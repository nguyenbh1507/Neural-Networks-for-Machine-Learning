In this video, I'm going to talk about 
the use of binary codes for image retrieval. 
For retrieving documents, people like Google have such good methods already, 
the techniques like semantic hashing may not be of much value. 
But retrieving images is much more difficult and the methods that convert an 
image into a fairly large binary code of say, 256 bits, seem to work quite well. 
However, we don't want to do a very long sequencial search through vectors of 256 
bits so semantic hashing can be used to first create a short list and then we can 
get better quality matches by using longer binary codes in a serial search. 
Now, we get to look at using binary codes for image retrieval. 
Image retrieval of present is typically done by using the captions. 
But why not use the images too? They obviously contain a lot more 
information than the captions. The basic problem is that pixels are not 
like words. Individual pixels don't tell us much 
about the content of an image. Obviously if you could recognize the 
objects in the images, then we'd have things that were much more like words. 
But recognizing objects is hard. At least it was when we first did this 
work. Now deep neural nets have got much better 
at it, and so that may well be the way to go. 
So if we're not going to recognize the objects, maybe what we should do is 
extract a vector that has information about the content of the image. 
And the obvious thing to extract is the real valued vector. 
But the problem is that matching real value vectors in the real data base is 
slow and it also requires a lot of storage for the real value vectors. 
If we can extract a fairly short binary vector that contains a lot of information 
about the image that's much easier to store and much faster to match, even 
faster is to use a two stage method. So first, we extract a short binary code 
of about 30 bids and that short binary code is used with semantic hashing to 
very rapidly get us a short list of promising images. 
So we simply take the short binary code, and flip a few bits in it to get 
candidate images. The candidate images can then be matched 
using 256 bit binary codes that are stored with each known image. 
To search for much better matches than can be found with a 28 bit binary code. 
Even a 256 bit binary code only requires four words of memory per image. 
And even though we're then going to do a serial on these binary codes. 
The search can be done very fast, because it only requires a few operations to 
compare two, 256 binary codes to find out how many bits they have in common. 
The question is, how good is a binary code of that size at retrieving images?. 
Are they going to find images that we think of as being similar? 
So here's a net that trained by Alex Krizhevsky. 
It's working on small color images so they're only 32 pixels by 32 pixels and 
he takes his input the red, green, and blue channels from those images so 3,000 
inputs. He then expands that to a larger number 
of hidden units because we're going to go from real value inputs to logistic hidden 
units, which probably have less capacity. 
We then progressively decrease the number units in each layer until we get down to 
256 bits. This encoder has about 67 million 
parameters. It's quite big. 
It takes a few days to train it on, in video GPU. 
And I'll exchange it on two million images. 
There's absolutely no theory to justify the architecture we used. 
We know we want a fairly deep net. It makes sense to make it get an arrow as 
we go up. But this particular architecture, where 
you have the number of units each layer, is just a guess. 
The interesting thing is, a guess like this already works quite well. 
And presumably, there are some other architectures that will work better. 
The first question to ask is how well does an ordering coder like this do at 
reconstructing the images? So, here is a face image and it's 
reconstruction and you can see that from the reconstruction you can tell the kind 
of an image it is. Here's another example, where it's a 
scene probably at a party, you can't really tell what kind of scene it is from 
the image but you might guess that there are a number of people involved or you 
might not. Here's an outdoor scene. 
And you can see that the reconstruction captures a lot of information about the 
accuracy. It captures the water in the sky and the 
hyphens of thin strip land. So lets look at the quality of 
retrievable we can do within ordering codes that gives those kinds of 
reconstruction. We'll start with a picture of Michael 
Jackson in the red square and Alex retrieved the most similar images and 
above each image you can see how many bits had differed from Michael Jackson 
body. And you'll notice they're all a fairly 
similar number of bits. In 256 bits, differing by only 61 bits is 
extraordinarily unlikely to happen by chance if they were random images. 
It has to be a pretty similar image to differ by so few bits. 
One nice thing about what's retrieved is, with one exception, they're all faces. 
If we look at the retrieval you get by using euclidean distance on the raw 
pixels, then some of them are faces, but most of 
them aren't. So obviously the ordering coder has 
understood something about faces that isn't in the information about Euclidean 
distance. It's clearly giving much better 
retrieval. Let's take another example, here we took 
the image of a party scene and we treat of the images. 
And you can see that about half of them have images you would think of as fairly 
similar that other party scenes. The tip of the other party scenes with 
something bright in the middle, like the oiginal party scene, and you'll also 
notice that most of the bad matches also have something bright in the middle. 
So even though we're getting down to 256 bit binary codes through a lot of hidden 
layers, it's still sensitive to quite a lot about the image structure and where 
the brighter patches are. If you look at what Euclidean distance 
does, it's much worse. Euclidean distance gets one other scene 
with a, a group of people. And then everything else is fairly 
dissimilar. You'll notice with Euclidean distance, it 
often gets very smooth images. That's because if you can't match the 
high frequency variation in the image, it's better to match its average then to 
get other stuff with high frequency variations out of phase. 
So when you got a complicated image, including in distance will typically find 
smooth images to match it. And that's because it's minimizing a 
squared error in pixel space. So obviously we'd like the image 
retrievable to be more sensitive to the content of the image that is stored kinds 
of objects in the relationships in image and less sensitive to the pixel 
intensities. We can do that by first training a big 
net to recognize lots of different kinds of object in real images and we show you 
how to do that in lecture five. Then we take the activity vector, in the 
last hidden layer of the big net, and use that as a representation of the image. 
This should be much better than the pixel intensities at capturing information 
about the kinds of objects in the image. So to see if this approach is likely to 
work, we use the net described in lecture five, the one the image that competition. 
So far, we've only tried it on Euclidan distance, between the activity of vectors 
in the last hidden layer. But obviously if it works for that, 
we could then that those activity vectors and build an ordering code around those 
to get them down to binary codes. So let's first see if it works with the 
Euclidean distance. It turns out it works really well. 
We don't know yet whether it will work with binary codes. 
So, in the column on the left, you see the query images. 
And then to the right of them, you see all the things that were retrieved. 
If you look at the elephant query image, you'll see that what gets retrieved is 
other elephants but elephants with very different poses. 
So those images wouldn't have a very good overlap in pixel space, although the 
overlap wouldn't be that bad. If you look at the Halloween pumpkins, 
you'll see that all the retrieved things are other Halloween pumpkins. 
And some of them, would have a pretty bad overlapping pixel space. 
Similar with the aircraft carriers, we retrieve other images of aircraft carrier 
that are very different. So we anticipate that if we could reduce 
the activity vector to short binary code. We would have a fast and effective way of 
retrieving similar images just by the content of the image. 
We'll see in lecture sixteen that we could actually combine the content of the 
image with the caption of the image, to get an even better representation.