In this video, I'm going to talk about 
alternative pre-training methods for learning deep neural nets. 
I introduced pre-training using restrictive Boltzmann machines trained 
with contrastive divergence. But after that, people discovered there 
are many other ways to pre-train layers of features. 
And indeed, if you initialize the weights correctly, you may not need pre-training 
at all provided you have enough labeled data. 
We've seen some of the neat things that can be done with the codes produced by 
deep auto-encoders. I now want to consider shallow 
auto-encoders that just have one hidden layer. 
Restricted Boltzmann machines can be used with shallow auto-encoders, particularly 
if they're trained with contrastive divergence because they're trying to make 
the reconstructions look like the data. When do you use an autoencoder? 
A restricted Boltzmann machine has very strong regularization because the hidden 
units are only allowed to have binary activities, 
and this restricts their capacity a lot. If we train restricted Boltzmann machines 
with maximum likelihood, they're not at all like auto-encoders. 
One way to see that is if you had a pixel that was pure noise, an auto-encoder 
would try to reconstruct whatever noise value it had. 
A restricted Boltzmann machine trained with maximum likelihood would completely 
ignore that pixel and model it just using the bias for that input. 
So, since we can view a restricted Boltzmann machine as a kind of a strongly 
regularized auto-encoder, maybe we can replace the RBMs that we use for 
pre-training with a stack of autoencoders. 
It turns out that if you do that, pre-training is not as effective. 
At least, that's true if you use shallow auto-encoders that are regularized just 
by penalizing the squared weights. So, stacking these autoencoders doesn't 
work as well as stacking restricted Boltzmann machines. 
However, there's a different kind of auto-encoder that does work as well, 
and that's the denoising auto-encoder. And, it's been studied extensively by the 
group in Montreal. Denoising auto-encoders work by adding 
noise to each input vector by setting many of the components to zero, but it's 
different components for different input vectors. 
This resembles dropout, but it's for the inputs rather than the hidden units. 
The denoising auto-encoder is still required to reconstruct, the inputs that 
have been set to zero. And so, it can't just copy its input. 
The danger with the shallow auto-encoder is that if you give it enough hidden 
units, it might just copy each pixel to one hidden unit, and then reconstruct 
that pixel from that hidden unit. A denoising auto-encoder clearly can't do 
that so it has to use hidden units to catch a correlation between inputs so 
that it can use the values of some inputs to help it reconstruct the inputs that 
have been zeroed out. If we use a stack of denoising 
auto-encoders, pre-training is very effective. 
There's some cases in which RBMs still work better, but in most cases denoising 
auto-encoders are more effective. It's also much simpler to evaluate the 
pre-training using a denoising autoencoder because we can easily compute 
the value of the objective function. When we pre-train a restricted Boltzmann 
machine with contrast divergence, we can't compute the value of the real 
objective function we're trying to minimize. 
So, we often just use the squared reconstruction error, which is not 
actually what's being minimized. In a denoising auto-encoder, we can print 
out the value of the thing we're trying to minimize, and that's very helpful. 
One disadvantage of the denoising autoencoder is that it lacks the nice 
variational boundary we get with restricted Boltzmann machines. 
But that's only of theoretical interest because it only applies if the restricted 
Boltzmann machine is trained with maximum likelihood. 
Yet another kind of auto-encoder is the contractive auto-encoder, that was also 
developed by the group in Montreal. The way this works is that we try to make 
the hidden activities be as insensitive as possible to the inputs. 
Of course, the hidden units can't just ignore the inputs altogether because they 
have to be able to reconstruct them. The way we achieve this insensitivity is 
by penalizing the squared gradient of each hidden unit with respect to each 
input. So, we try to make each hidden unit so that it won't change much if we 
change an input value. Contractive auto-encoders also work very 
well for pre-training. Their codes tend to have the property 
that only a small subset of the hidden units are in their sensitive range. 
For different parts of the space, it's a different subset and so this active set 
acts like a sparse code. The other hidden units are unsaturated 
and are insensitive. RBMs actually have a very similar 
behavior. After they've been trained, many of the 
hidden units will be saturated, and the working set of the unsaturated ones will 
be different for different training cases. 
I want to finish by summarizing my current view of pre-training. 
There are now many different ways to do layer by layer pre-training that 
discovers good features. When our data set does not have a huge 
number of labels, this way of discovering features before 
you ever use the labels is very helpful for the subsequent discriminative fine 
tuning. It discovers the features without using 
the information in the labels, and then the information in the labels is used for 
fine tuning the decision banquets between classes. 
It's especially useful if we have a lot of unlabeled data so that the 
pre-training can be a very good job of discovering interesting features, using a 
lot of data. For very large labeled data sets however, 
initializing the weights that are going to be used for supervised learning by 
using unsupervised pre-training is not necessary, even if the nets are deep. 
Pre-training was the first good way to initialize the weights for deep nets, but 
now we have lots of other ways. However, even if we have a lot of labels, 
if we make the nets much larger again, we'll need pretraining again. 
So, an argument I often have with people from Google is they say, we've got lots 
and lots of labelled data so we don't need regularization methods. 
Our nets won't over fit anyway because we've got so much data. 
The counter-argument is, that's only because you're using nets that are much 
too small. You should use much, much bigger nets on 
much, much more powerful computers. And then, you'll start over fitting again 
and you'll need these regularization methods, like dropout and pre-training. 
If you ask which regime the brain is in, the brain is clearly in the regime where 
it got huge numbers of parameters compared with the amount of data its got. 
And so to the brain, at least, regularization methods are very 
important.