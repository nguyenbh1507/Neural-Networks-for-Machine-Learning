In this video, I'm going to talk about the
storage capacity of Hopfield nets. Their ability to store a lot of memories
is limited by what are called spurious memories.
These occur when two nearby energy minima combine to make a new minimum in the wrong
place. Attempts to remove these spurious minima
eventually led to a very interesting way of doing learning in things considerably
more complicated than a basic Hopfield net.
At the end of the video, I'll also talk about a curious historical
rediscovery where the physicist trying to increase the capacity of Hopfield nets,
rediscovered the perceptron convergence procedure.
Off to Hopfield, invented Hopfield nets as memory storage devices.
The field became obsessed by the storage capacity of a Hopfield net.
Using Hopfield Storage Rule for a totally connected net, the capacity is about 0.15N
memories. That is, if you have N binary threshold
units, the number of memories you can store is about 0.15N before memories start
getting confused with one another. So that's the number you can store and
still hope to retrieve them sensibly. Each memory is a random configuration of
the N units, so it has N bits of information it.
And so, the total information being stored, in a Hopfield net is about 0.15N
squared bits. This doesn't make efficient use of the
bits that are required to store the weights.
In other words, if you look at how many bits the computer is using to store the
weights, it's using well over 0.15N squared2 bits to store the weights.
And therefore, this kind of distributed memory and local energy minima is not
making efficient use of the bits in the computer.
We can analyze how many bits we should be able to store if we were making efficient
use of the bits in the computer. Those N squared weights and biases in the
net. And after storing M memories, each
connection weight has an integer value in the range -M to M. That's because we
increase it by one or decrease it by one each time we store a memory, assuming that
we used states of -one and one. Now, of course, not all values will be
equiprobable, so we could compress that information.
But ignoring that, the number bits it would take us to store a connection rate
in a naive way is log 2M + one, Cuz that's the number of alternative connection rates
and that's a log to the base two. And so, the total number of bits of
computer memory that we use is of the order of N squared log 2M + one.
So, notice that, that scales logarithmically with M.
Whereas, if you store things in the way that Hopfield suggests, you get this
constants 0.15 instead of something this scale is logarithmically.
So, we're not so worried about the fact that the constant is a lot less than two,
What we're worried about is this logarithmic scaling.
That shows we ought to be able to do something better.
If we ask, what limits the capacity of a Hopfield net? What is it that causes it to
break down? Then, its merging of energy minima.
So, each time we memorize a binary configuration, we hope that we'll create a
new energy minima. So, we might have a state space for all
the states of the net being depicted horizontally here, and the energy being
depicted vertically. And, we might have one en, energy minimum
for the blue pattern and another for the green pattern.
But, if those two patents are nearby, what will happen is we won't get two seperate
minima. They'll merge to create one minimum at an intermediate location. And
that means, we can't distinguish those two seperate memories, and indeed we'll recall
something, that's a blend of them rather than the individual memories.
That's what limits the capacity of a Hopfield net, that kind of merging of
nearby minima. One thing I should mention is this
picture, is a big misrepresentation. The states of a Hopfield matter really, the
corners of a hyper cube. And, it's not very good to show, the corners of a hyper
cube, as if they were continous one dimensional horizontal space.
One very interesting idea that came out of thinking about how to improve the crest of
the Hopfield net is the idea of unlearning.
This was first suggested by Hopfield, Feinstine and Palmer, who suggested the
following strategies. You left a net settle from a random
initial state, and then you do unlearning. That is whatever binary state it settles
to, you apply opposite of the storage rule.
I think you can see that with the previous example, that red merge minimum.
If you let the net settle there and did some unlearning on that merge minimum,
you'd get back the two separate minima cuz you'd pull up that red point.
So, by getting rid of deep spurious minima, we can actually increase the
memory capacity. Hopfield, Feinstein and Palmer showed that
this actually worked, but they didn't have a good analysis of what was really going
on. Francis Crick, one of the discovers of the
structure of DNA, and Graham Micherson, proposed that unlearning might be what's
going on during REM sleep, that is Rapid Eye Movement sleep.
So, the idea was that during the day, you store lots of things, and you'll get
spurious minima. Then at night, you put the network in a
random state, you settle to a minimum, And you unlearn what you settled to.
And that actually explains a big puzzle. This is a puzzle that doesn't seem to
puzzle most people that study sleep but it ought to.
Each night, you go to sleep and you dream for several hours. When you wake up in the
morning, those dreams are all gone. Well, they're not quite all gone. The dream you
had just before you woke up, you can get into short term memory and you'll remember
it for a while. And if you think about it, you might remember it for a long time.
But, we know perfectly well that if we'd woken you up at other times in the night,
you'd have been having other dreams, and in the morning their just not there.
So, it looks like you're simply not storing what you're dreaming about, and
the question is, why? In fact, why do you bother to dream at all?
Dreaming is paradoxical and that the state of your brain looks extremely like the
state of your brain when you're awake, except that it's not being driven by real
input. It's being driven by a relay station just after the real input called
the thalamus. So the Crick and Mitchison theory at least
explains, functionally, what the point of dreams is, is to get rid of the spurious
minima. But, there's another problem with
unlearning, which is more mathematical problem, Which is, how much unlearning
should we do? Now, given more you've seen in the school
so far, a real solution to that problem will be to show that unlearning is part of
the process of fitting a model to data. And, if you do maximum likelihood fitting
of that model, then unlearning will automatically come out and fit into the
model. And what's more, you'll know exactly how
much unlearning to do. So, what we're going to try and do is
derive on learning as the right way to minimize a cost function.
Where the cost function is, how well your neural net models the data that you saw
during the day. Before we get to that, I want to talk a
little bit about ways that physicists discovered for increasing the capacity of
the Hopfield net. As I said, this was a big obsession with
the field. I think it's because physicists really
love the idea that math they already know might explain how the brain works.
That means, post doctoral fellows in physics who can't get a job in physics
might be able to get a job in neuroscience.
So, there are a very large number of papers published in physics journals about
Hopfield and their storage capacity. Eventually, a very smart student called,
Elizabeth Gardner, figured out that there's actually a much better storage
rule if you were concerned about capacity. And that it would use the full capacity of
the weights. And I think this storage rule will be
familiar to you. Instead of trying to store vectors in one
go, what we're going to do is we're going to cycle through the training set many
times. So, we lose our nice online property that you only have to go through
the data once. But in return, we're going to gain, more efficient storage.
What we going to do is we going to use the perceptual convergent procedure to train
each unit to have the correct state given the states of all the other units in that
global vector that we want to store. So, you take your net, you put it into the
memory state you want to store, and then you take each unit separately and say,
would this unit adopt the state I want for it, given the states of all the other
units? If it would, you leave its incoming
weights alone. If it wouldn't, you change its incoming
weights in the weights specified by convergence procedures. And notice, these
would be integer changes to the weights. You may have to do this several times, and
of course, if you give it to many memories, this won't converge. You only
get convergence with a perceptron convergence procedure if there is a set of
weights that will solve the problem. But assuming there is, this is much more
efficient way to store memories in a Hopfield net.
This technique is also being developed in another field, statistics.
And statisticians call the technique pseudo-likelihood.
The idea is to get one thing right given all the other things.
And so, with high dimensional data, if you want to build a model of it, the idea is
you build a model that tries to get the value on one dimension right given the
values on all the other dimensions. The main difference between the perceptron
convergence procedure that's normally used and pseudo-likelihood is that, in the
Hopfield net, the weights are symmetric. So, we have to get two sets of gradients
for each weight and average them. But, apart from that, the way to use the
full capacity of a Hopfield net is to use the perceptron convergence procedure and
to go through the data several times.