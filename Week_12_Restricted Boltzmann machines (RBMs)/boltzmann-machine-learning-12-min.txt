In the previous video,
I showed how a Boltzmann machine can be used a probabilistic model of a set of
binary data vectors. In this video we're finally going to get
around to the Boltzmann machine learning algorithm.
This is a very simple learning model which has an elegant theoretical justification,
but it turned out in practice, it was extremely slow and noisy, and just wasn't
practical. And for many years, people thought that
Boltzmann machines would never be practical devices.
Then we found several different ways of greatly speeding up the learning
algorithm. And now the algorithm is much more
practical, and has, in fact, been used as part of the winning entry for a million
dollar machine learning competition, which I'll talk about in a later video.
The Bolton machine learning algorithm is an unsupervised learning algorithm.
Unlike the typical user back propagation, where we have a input vector and we
provide it with a desired output. In Boltzmann machine learning we just give it
the input vector. There are q labels. What the algorithm is
trying to do is build a model of a set of input vectors, though it might be better
to think of them as output vectors. What we want to do is maximize the product
of the probabilities, that the Boltzmann machine assigns to a set of binary
vectors, The ones in the training set.
This is equivalent to maximizing the sum of the log probabilities that the
Boltzmann machine assigns to the training vectors.
It's also equivalent to maximizing the probability that we'd obtain exactly the
end training cases, if we ran the Boltzmann machine in the following way.
First, we let it settle to its stationary distribution, and different times, with no
external input. Then we sample the visible vector once.
Then we let it settle again, and sample the visible vector again.
And so on. Now the main reasons why the learning
could be difficult. This is probably the most important
reason. If you consider a chain of units,
A chain of hidden units here, with visible units attached to the two ends,
And if we use a training set that consist of one, zero and zero, one.
In other words, we want the two visible units to be in opposite states.
Then the way to achieve that is by making sure that the product of all those weights
is negative. So, for example, if all of the weights are
positive, turning on W1 will tend to turn on the first hidden unit.
And that will tend to turn on the second hidden unit, and so on.
And the fourth hidden unit will tend to turn on the other visible unit.
If one of those weights is negative, then we'll get an anti-correlation between the
two visible units. What this means is, that if we're thinking
about learning weight W1, we need to know other weights.
So there's W1. To know how to change that weight, we need
to know W3. We need to have information about W3,
because if W3 is negative what we want to do with W1 is the opposite of what we want
to do with W1 if W3 is positive. So given that one weight needs to know
about other weights in order to be able to change even in the right direction, it's
very surprising that there's a very simple learning algorithm, and that the learning
algorithm only requires local information. So it turns out that everything that one
weight needs to know about all the other weights and about the data is contained in
the difference of two correlations. Another way of saying that is that if you
take the log probability that the Boltzmann machine assigns to a visible
vector V. And ask about the derivative of that log
probability with respect to a weight, WIJ. It's the difference of the expected value
of the products of the states of I and J. When the networks settle to thermal
equilibrium with v clamped on the visible units.
That is how often are INJ on together when V is clamped in visible units and the
network is at thermal equilibrium, minus the same quantity.
But when V is not clamped on visible units, so because the derivative of the
log probability of a visible vector is this simple difference of correlations we
can make the change in the weight be proportional to the expected product of
the activities average over all visible vectors in the training set, that's what
we call data. Minus the product of the same two
activities when your not clamping anything and the network has reached thermal
equilibrium with no external interference. So this is a very interesting learning
rule. The first term in the learning rule says
raise the weights in proportion to the product of the activities the units have
when you're presenting data. That's the simplest form of what's known
as a Hebbian learning rule. Donald Hebb, a long time ago, in the 1940s
or 1950s, suggested that synapses in the brain might use a rule like that.
But if you just use that rule, the synapse strengths will keep getting stronger.
The weights will all become very positive, and the whole system will blow up.
You have to somehow keep things under control, and this learning algorithm is
keeping things under control by using that second term.
It's reducing the weights in proportion to how often those two units are on together,
when you're sampling from the model's distribution.
You can also think of this as the first term is like the storage term for a
Hopfield Net. And the second term is like the term for
getting rid of spurious minima. And in fact this is the correct way to
think about that. This rule tells you exactly how much
unlearning to do. One obvious question is why is the
derivative so simple. Well, the probability of a global
configuration at thermal equilibrium, that is once you've let it settle down, is an
exponential function of its energy. The probability is related to E to the
minus energy. So when we settle to equilibrium we
achieve a linear relationship between the log probability and the energy function.
Now, the energy function is linear in the weights.
So, we have a linear relationship between the weights and the log probability.
And since we're trying to manipulate log probabilities by manipulating weights,
that's a good thing to have. It's a log linear model.
In fact, the relationship's very simple. It's that the derivative of the energy
with respect to a particular weight WIJ is just the product of the two activities
that, that weight connects. So what's happening here?
Is the process of settling to thermal equilibrium is propagating information
about weights? We don't need an explicit back propagation
stage. We do need two stages.
We need to settle with the data. And we need to settle with no data.
But notice that the networks behaving in pretty much the same way in those two
phases. The unit deep within the network is doing
the same thing, just with different boundary conditions.
With back prop the forward pass and the backward pass are really rather different.
Another question you could ask is what's that negative phase for.
I've already said it's like the unlearning we do in a Hopfield net to get rid of
spurious minima. But let's look at it in more detail.
The equation for the probability of a visible vector, is that it's a sum overall
hidden vectors of E to the minus the energy of that visible and hidden vector
together. Normalized by the same quantity, summed
overall visible vectors. So if you look at the top term, what the
first term in the learning rule is doing is decreasing the energy of terms in that
sum that are already large and it finds those terms by settling to thermal
equilibrium with the vector V clamped so that it can find an H that goes nicely
with V, that is gives a nice low energy with V.
Having sampled those vectors H, it then changes the weights to make that energy
even lower. The second phase in the learning, the
negative phase, is doing the same thing, but for the partition function.
That is, the normalizing term on the bottom line.
It's finding global configurations, combinations of visible and hidden states
that give low energy, And therefore, are large contributors to
the partition function. And having find those global
configurations, it tries to raise their energy so that the can contribute less.
So the first term is making the top line big, and the second term is making the
bottom line small. Now in order to run this learning rule,
you need to collect those statistics. You need to collect what we call the
positive statistics, those are the ones when you have data clamped on the visible
units, and also the negative statistics, those are the ones when you don't have
data clamped and that you're going to use for unlearning.
An inefficient way to track these statistics was suggested by me and Terry
Sejnowski in 1983. And the idea is, in the positive phase you
clamp a data vector on the visible units, you set the hidden units to random binary
states, And then you keep updating the hidden
units in the network, one unit at a time, until the network reaches thermal
equilibrium at a temperature of one. We actually did that by starting at a high
temperature and reducing it, but that's not the main point here.
And then once you reach thermal equilibrium, you sample how often two
units are on together. So you're measuring the correlation of INJ
with that visible vector clamped. You then repeat that, over all the visible
vectors, so that, that correlation you're sampling is averaged over all the data.
Then in the negative phase, you don't clamp anything.
The network is free from external interference.
So, you set all of the units, both visible and hidden, to random binary states.
And then you update the units, one at a time, until the network reaches thermal
equilibrium, at a temperature of one. Just like you did in the positive phase.
And again, you sample the correlation of every pair of units INJ,
And you repeat that many times. Now it's very difficult to know how many
times you need to repeat it, but certainly in the negative phase you expect the
energy landscape to have many different minima, but are fairly separated and have
about the same energy. The reason you expect that is we're going
to be using Boltzmann machines to do things like model a set of images.
And you expect there to be reasonable images, all of which have about the same
energy. And then very unreasonable images, which
have much higher energy. And so you expect a small fraction of the
space to be these low energy states. And a very large fraction of the space to
be these bad high energy states. If you have multiple modes, it's very
unclear how many times you need to repeat this process to be able to sample those
modes.