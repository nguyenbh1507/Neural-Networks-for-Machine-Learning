In this video, I'm going to show a simple
example of a restricted Boltzmann machine learning a model of images of handwritten
twos. After it's learned the model, we'll look
at how good it is at reconstructing twos. And we'll look at what happens if we give
it a different kind of digit and ask it to reconstruct that.
We'll also look at the weights we get if we train a restricted Boltzmann machine
that's considerably larger on all of the digit classes.
It lends a wide variety of features, which between them are very good at
reconstructing all the different classes of digits, and also are quite a good model
of those digit classes. That is, if you take a binary vector, this
image of a hundred digit, the model will be able to find low energy states,
compatible with that image and if you give it an image that's a long way away from
being an image of a hundred digit, the model will not be able to find low energy
states compatible with that image. I'm now gonna show how a relatively simple
RBM can learn to build a model of images of the digit two.
The images of sixteen pixels by sixteen pixels.
And it has 50 binary hidden units that they're gonna learn to become interesting
feature detectors. So when it's presented with the data case,
the first thing that it does is use the weight and the connection from pixel to
feature like this, to activate the features like this.
That is for each of the binary neurons, it makes the sarcastic decision about whether
it should deduct the state of one or zero. It then uses the binary pans for
activation to reconstruct the data. That is, for each pixel, it makes a binary
decision about whether it should be a one or a zero.
It then reactivates the binary feature detectors using the reconstruction to
activate them rather than the data. The weights are changed by incrementing
the weights between an active pixel and an active feature detector when the network
is looking at data. And that will lower the energy of the
global configuration of the data, and whatever hidden pattern went with it.
And it decrements the weights, between an active pixel and an active feature
detector, when it's looking at a reconstruction, and that would raise the
energy of the reconstruction. Near the beginning of learning when the
weights are random, the reconstruction would almost certainly have lower energy
than the data. Because the reconstruction is what the
network likes to reproduce on the visible units, given the hidden pattern of
activity. And obviously it likes to reproduce
patterns that have low energy according to its energy function.
And you can think of what learning does as changing the weights so the data is low
energy. And the reconstructions of the data are
generally higher energy. So let's start with some random weights
for the 50 feature detectors. We'll use small random weights and each of
these squares shows you the weights to the pixels coming from a particular feature
detector. The small random weights are used to break
symmetry. That because the update were stochastic,
we don't really need that. After seeing a few hundred examples of
digits, and digesting the weights a few times, the weights are beginning to form
patterns. If we do it again, you can see that many
of the feature detectors are detecting the pattern of a hole two.
They're fairly global feature detectors. And those feature detectors are getting
stronger, And stronger, and now some of them begin
to localize, and they're getting more local, and even more local, and even more
local, and these are the final weights and you can see that each neuron has become a
different feature detector and most of the feature detectors are fairly local.
If you look at the feature detector in the red box, for example, it's detecting the
top of a two, And it's happy when the top of a two is
where its white pixels are and where there's nothing where the black pixels
are. So it's representing where the top of the
two is. Once we've learned the model, we can look
at how well it reconstructs digits. And we'll give it some test digits that it
hasn't seen before. So we'll start by giving it a test example
of a two. And its reconstruction is pretty faithful
to the test example. It's slightly blurry.
The test example has a hook at the top and that's been blurred after the
reconstruction, but it's a pretty good reconstruction.
The more interesting thing we can do, is give it a test example from a different
digit class. So if we give it an example of the three
to reconstruct. What it reconstructs actually looks more
like a two then like a three. All of the feature detectors is learned,
are good for representing twos, but it doesn't have feature detectors for things
like representing that cusp in the middle of the three.
So it ends up reconstructing something, but obeys the regularities of a two,
better than it represents the regularities of a three.
In fact, the network tries to see everything as a two.
So here's some feature detectors that were learned in the first hidden layer of a
model that uses 500 hidden units to model all ten digit classes.
And this model has been trained for a long time with contrastive divergence.
It has a big variety of feature detectors. If you look at the one in the blue box,
that's obviously going to be useful for detecting things like eights If you look
at the one in the red box, it's not what you expect to see.
It likes to see pixels on very near the bottom there, and it really doesn't like
to see pixels on in a road that's 21 pixels above the bottom.
What's going on here is that the data is normalized and so the digits can't have a
height of greater than twenty pixels. And that means if you know that there's a
pixel on where those big positive weights are, there can't possibly be a pixel on,
where those negative weights are. So this is picking up on the long range
regularity that was introduced by the way we normalized the data.
Here's another one that's doing the same thing for the fact that the data can't be
wider than twenty pixels. The feature detected in the green box is
very interesting. It's for detecting where the bottom of a
vertical stroke comes and it will detect it in a number of different positions and
then refuse to detect it in the intermediate positions.
So it's very like one of the least significant digits in a binary number, as
you increase the magnitude of a number it goes on again, and off again, and on
again, and off again and it shows that this is developing quite complex ways of
representing where things are.