WEBVTT

1
00:00:00.000 --> 00:00:05.672
In this video, I'm going to talk about 
some recent work on learning a joint 

2
00:00:05.672 --> 00:00:10.600
model of captions and feature vectors 
that describe images. 

3
00:00:10.600 --> 00:00:15.404
In the previous lecture, I talked about 
how we might extract semantically 

4
00:00:15.404 --> 00:00:20.209
meaningful features from images. 
But we were doing that with no help from 

5
00:00:20.209 --> 00:00:23.828
the captions. 
Obviously the words in a caption ought to 

6
00:00:23.828 --> 00:00:28.370
be helpful in extracting appropriate 
semantic categories from images. 

7
00:00:28.370 --> 00:00:33.503
And similarly, the images ought to be 
helpful in disambiguating what the words 

8
00:00:33.503 --> 00:00:38.004
in the caption mean. 
So the idea is we're going to try in a 

9
00:00:38.004 --> 00:00:43.243
great big net that gets its input, stand 
to computer vision feature vectors 

10
00:00:43.243 --> 00:00:48.762
extractive for images and pack up words 
representations of captions and learns 

11
00:00:48.762 --> 00:00:52.954
how the two input representations are 
related to each other. 

12
00:00:52.954 --> 00:00:58.682
At the end of the video I'll show you a 
movie of the final network using words to 

13
00:00:58.682 --> 00:01:04.271
create feature vectors for images and 
then showing you the closest image in its 

14
00:01:04.271 --> 00:01:07.953
data base. 
And also using images to create bytes of 

15
00:01:07.953 --> 00:01:11.227
words. 
I'm now going to describe some work by 

16
00:01:11.227 --> 00:01:16.830
Natish Rivastiva, who's one of the TAs 
for this course, and Roslyn Salakutinov, 

17
00:01:16.830 --> 00:01:21.439
that will appear shortly. 
The goal is to build a joint density 

18
00:01:21.439 --> 00:01:27.017
model of captions and of images except 
that the images represented by the 

19
00:01:27.017 --> 00:01:33.349
features standardly used in computeration 
rather than by the ropic cells.This needs 

20
00:01:33.349 --> 00:01:39.304
a lot more computation than building a 
joint density model of labels and digit 

21
00:01:39.304 --> 00:01:42.470
images which we saw earlier in the 
course. 

22
00:01:42.470 --> 00:01:46.749
So what they did was they first trained a 
multi-layer model of images alone. 

23
00:01:46.749 --> 00:01:51.028
That is it's really a multi-layer model 
of the features they extracted from 

24
00:01:51.028 --> 00:01:54.460
images using the standard computer vision 
features. 

25
00:01:54.460 --> 00:02:00.461
Then separately, they train a multi-layer 
model of the word count vectors from the 

26
00:02:00.461 --> 00:02:03.724
captions. 
Once they trained both of those models, 

27
00:02:03.724 --> 00:02:07.736
they had a new top layer, that's 
connected to the top layers of both of 

28
00:02:07.736 --> 00:02:11.954
the individual models. 
After that, they use further joint 

29
00:02:11.954 --> 00:02:17.537
training of the whole system so that each 
modality can improve the earlier layers 

30
00:02:17.537 --> 00:02:21.826
of the other modality. 
Instead of using a deep belief net, which 

31
00:02:21.826 --> 00:02:27.068
is what you might expect, they used a 
deep Bolton machine, where the symmetric 

32
00:02:27.068 --> 00:02:32.727
connections bring in all pairs of layers. 
The further joint training of the whole 

33
00:02:32.727 --> 00:02:37.178
deep Boltzmann machine is then what 
allows each modality to change the 

34
00:02:37.178 --> 00:02:41.560
feature detectors in the early layers of 
the other modality. 

35
00:02:41.560 --> 00:02:44.934
That's the reason they used a deep 
Boltzmann machine. 

36
00:02:44.934 --> 00:02:49.965
They could've also used a deep belief 
net, and done generative fine tuning with 

37
00:02:49.965 --> 00:02:53.912
contrastive wake sleep. 
But the fine tuning algorithm for deep 

38
00:02:53.912 --> 00:02:59.268
Boltzmann machines may well work better. 
This leaves the question of how they 

39
00:02:59.268 --> 00:03:02.392
pretrained the hidden layers of a deep 
Boltzmann machine. 

40
00:03:02.392 --> 00:03:06.393
because what we've seen so far in the 
course is that if you train a stack of 

41
00:03:06.393 --> 00:03:10.449
restricted Boltzmann machines and you 
combine them together into a single 

42
00:03:10.449 --> 00:03:15.520
composite model what you get is a deep 
belief net not a deep Boltzmann machine. 

43
00:03:15.520 --> 00:03:20.918
So I'm now going to explain how, despite 
what I said earlier in the course, you 

44
00:03:20.918 --> 00:03:25.601
can actually pre-trail a stack of 
restrictive Boltzmann machines in such a 

45
00:03:25.601 --> 00:03:29.828
way that you can then combine them to 
make a deep Boltzmann machine. 

46
00:03:29.828 --> 00:03:35.162
The trick is that the top and the bottom 
restrictive bowser machines in the stack 

47
00:03:35.162 --> 00:03:40.500
have to trained with weights that it 
twices begin one directions the other. 

48
00:03:40.500 --> 00:03:44.881
So, the bottom Boltzmann machine, that 
looks at the visible units is trained 

49
00:03:44.881 --> 00:03:48.863
with the bottom up weights being twice as 
big as the top down weights. 

50
00:03:48.863 --> 00:03:51.367
Apart from that, the weights are 
symmetrical. 

51
00:03:51.367 --> 00:03:54.180
So, this is what I call scale 
symmetrical. 

52
00:03:54.180 --> 00:03:57.487
But the bottom up weights are always 
twice as big as their top down 

53
00:03:57.487 --> 00:04:01.816
counterparts. 
This can be justified, and I'll show you 

54
00:04:01.816 --> 00:04:07.122
the justification in a little while. 
The next restrictive Boltzmann machine in 

55
00:04:07.122 --> 00:04:10.161
the stack, is trained with symmetrical 
weights. 

56
00:04:10.161 --> 00:04:14.851
I've called them two W, here rather then 
W for reasons you'll see later. 

57
00:04:14.851 --> 00:04:20.136
We can keep training with restrictive 
bowsler machines like that with genuinely 

58
00:04:20.136 --> 00:04:23.967
symmetrical weights. 
But then the top one in the stack has 

59
00:04:23.967 --> 00:04:28.790
be-trained with the bottom up weights 
being half of the top down weights. 

60
00:04:28.790 --> 00:04:33.554
So again, these are scale symmetric 
weights, but now, the top down weights 

61
00:04:33.554 --> 00:04:36.333
are twice as big as the bottom up 
weights. 

62
00:04:36.333 --> 00:04:41.561
That's the opposite of what we had when 
we trained the first restricted Bolton 

63
00:04:41.561 --> 00:04:45.135
machine in the stack. 
After having trained these three 

64
00:04:45.135 --> 00:04:50.296
restricted Bolton machines, we can then 
combine them to make a composite model, 

65
00:04:50.296 --> 00:04:55.656
and the composite model looks like this. 
For the restricted Bolton machine in the 

66
00:04:55.656 --> 00:05:00.620
middle, we simply halved its weights. 
That's why they were 2W2 to begin with. 

67
00:05:01.660 --> 00:05:05.773
For the one at the bottom, we've halved 
the up-going weights but kept the 

68
00:05:05.773 --> 00:05:09.606
down-going weights the same. 
And for the one at the top we've halved 

69
00:05:09.606 --> 00:05:13.520
the down-going weights and kept the 
up-going weights the same. 

70
00:05:13.520 --> 00:05:18.746
Now the question is: Why do we do this 
funny business of halving the whites? 

71
00:05:18.746 --> 00:05:24.181
The explanation is quite complicated but 
I'll give you a rough idea of what's 

72
00:05:24.181 --> 00:05:27.180
going on. 
If you look at the layer H1. 

73
00:05:27.180 --> 00:05:32.545
We have two different ways of inferring 
the states of the units in h1, in the 

74
00:05:32.545 --> 00:05:36.580
stack of restricted bolts and machines on 
the left. 

75
00:05:36.580 --> 00:05:42.125
We can either infer the states of H1 
bottom up from V or we can infer the 

76
00:05:42.125 --> 00:05:47.595
states of H1 top down from H2. 
When we combine these Boltzmann machines 

77
00:05:47.595 --> 00:05:53.665
together, what we're going to do is we're 
going to an average of those two ways of 

78
00:05:53.665 --> 00:05:58.434
inferring H1. 
And to take a geometric average, what we 

79
00:05:58.434 --> 00:06:04.212
need to do, is halve the weights. 
So we're going to use half of what the 

80
00:06:04.212 --> 00:06:07.711
bottom up model says. 
So that's half of 2W1. 

81
00:06:07.711 --> 00:06:12.593
And we're going to use half of what the 
top down model says. 

82
00:06:12.593 --> 00:06:16.689
That's half of 2W2. 
And if you look at the deep Boltzmann 

83
00:06:16.689 --> 00:06:20.988
machine on the right, that's exactly 
what's being used to infer the state of 

84
00:06:20.988 --> 00:06:23.137
H1. 
In other words, if you're given the 

85
00:06:23.137 --> 00:06:27.606
states in H2, and you're given the states 
in V, those are the weights you'll use 

86
00:06:27.606 --> 00:06:32.832
for inferring the states of H1. 
The reason we need to halve the weights 

87
00:06:32.832 --> 00:06:38.439
is so that we don't double count. 
You see, in the Boltzmann machine on the 

88
00:06:38.439 --> 00:06:41.882
right. 
The state of H2 already depends on V. 

89
00:06:41.882 --> 00:06:47.074
At least it does after we've done some 
settling down in the Boltzmann Machine. 

90
00:06:47.074 --> 00:06:51.999
So if we were to use the bottom up input 
coming from the first restricted 

91
00:06:51.999 --> 00:06:56.858
Boltzmann Machine in the stack. 
And we use the top down input coming from 

92
00:06:56.858 --> 00:07:01.651
the second Boltzmann Machine in the 
stack, we'd be counting the evidence 

93
00:07:01.651 --> 00:07:06.909
twice.'Cause we'd be inferring H1 from V. 
And we'd also be inferring it from H2, 

94
00:07:06.909 --> 00:07:10.834
which, itself, depends on V. 
In order not to double count the 

95
00:07:10.834 --> 00:07:15.761
evidence, we have to halve the weights. 
That's a very high level and perhaps not 

96
00:07:15.761 --> 00:07:19.483
totally clear description of why we have 
to half the weights. 

97
00:07:19.483 --> 00:07:24.400
If you want to know the mathematical 
details, you can go and read the paper. 

98
00:07:24.400 --> 00:07:28.359
But that's what's going on. 
And that's why we need to halve the 

99
00:07:28.359 --> 00:07:31.313
weights. 
So that the intermediate layers can be 

100
00:07:31.313 --> 00:07:36.152
doing geometric averaging of the two 
different models of that layer, from the 

101
00:07:36.152 --> 00:07:40.300
two different restricted Boltzmann 
machines in the original stack. 