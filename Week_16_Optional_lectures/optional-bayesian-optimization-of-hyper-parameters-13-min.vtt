WEBVTT

1
00:00:00.000 --> 00:00:05.018
In this video, I'm going to talk about 
some exciting recent work, which I think 

2
00:00:05.018 --> 00:00:09.779
will go a long way towards answering the 
question how do you settle those 

3
00:00:09.779 --> 00:00:15.933
hyper-parameters in a neural network? 
This recent work uses a different kind of 

4
00:00:15.933 --> 00:00:22.260
machine learning to help us decide what 
values to use for hyper-parameters. 

5
00:00:22.260 --> 00:00:26.502
In other words, it's using machine 
learning to replace the graduate student 

6
00:00:26.502 --> 00:00:30.236
who fiddles around with all these 
different settings of the 

7
00:00:30.236 --> 00:00:35.256
hyper-parameters to find out what works. 
It relies on a way of modeling smooth 

8
00:00:35.256 --> 00:00:39.390
functions called Gaussian processes, 
which I had always thought of as 

9
00:00:39.390 --> 00:00:44.063
inadequate for doing things like speech 
and vision and I still think they are 

10
00:00:44.063 --> 00:00:47.538
inadequate for that. 
But when you're in a domain where you 

11
00:00:47.538 --> 00:00:52.391
don't have much prior knowledge and the 
only thing that you can really appeal to 

12
00:00:52.391 --> 00:00:57.304
is that you expect similar inputs to have 
similar outputs, then Gaussian processes 

13
00:00:57.304 --> 00:01:00.419
are ideal. 
And that's the domain we're in when we're 

14
00:01:00.419 --> 00:01:04.673
fiddling around with vectors of 
hyper-parameters hoping to find a vector 

15
00:01:04.673 --> 00:01:09.168
of hyper-parameters that works well. 
So, for example is the number of hidden 

16
00:01:09.168 --> 00:01:12.233
units, the number of layers, the weight 
penalty, 

17
00:01:12.233 --> 00:01:17.496
whether it's used drop out or not. Those 
are all hyper-parameters and different 

18
00:01:17.496 --> 00:01:22.826
combinations of them work well together. 
So this is a very hard space to explore 

19
00:01:22.826 --> 00:01:26.404
by hand. 
It's very easy when we're exploring by 

20
00:01:26.404 --> 00:01:31.083
hand to fail to notice things. 
Gaussian processes are very good at 

21
00:01:31.083 --> 00:01:36.542
noticing trends in the data and they 
provide a very good way of finding good 

22
00:01:36.542 --> 00:01:40.300
sets of hyper-parameters if you have 
enough computers. 

23
00:01:41.400 --> 00:01:46.575
One of the commonest reasons that people 
give for not using neural networks is 

24
00:01:46.575 --> 00:01:50.408
that it requires a lot of skill to set 
the hyper-parameters. 

25
00:01:50.408 --> 00:01:55.327
This is actually a pretty good reason. 
If you don't have much experience, it's 

26
00:01:55.327 --> 00:01:59.544
easy to get stuck using a completely 
wrong value for one of the 

27
00:01:59.544 --> 00:02:04.619
hyper-parameters, and then nothing works. 
You have to set things like the number of 

28
00:02:04.619 --> 00:02:06.838
layers, 
the number of units per layer, 

29
00:02:06.838 --> 00:02:10.676
what types of units to use, 
the weight penalty, the learning rate, 

30
00:02:10.676 --> 00:02:14.935
the momentum, and so on and so on. 
If you use a learning rate that's 100 

31
00:02:14.935 --> 00:02:19.013
times too big or 100 times too small, 
your network simply won't work. 

32
00:02:19.013 --> 00:02:22.252
One way to approach this is to do a naive 
grid search. 

33
00:02:22.252 --> 00:02:25.911
That is, for each of these 
hyper-parameters, you make a list of 

34
00:02:25.911 --> 00:02:30.230
alternative values and then you try all 
possible combinations of values. 

35
00:02:30.230 --> 00:02:32.630
You can see that this is going to blow 
up. 

36
00:02:32.630 --> 00:02:35.276
If you have more than a few 
hyper-parameters, 

37
00:02:35.276 --> 00:02:39.830
you're going to end up with many more 
combinations than you can possibly try. 

38
00:02:39.830 --> 00:02:44.115
It turns out that there's something 
that's considerably better than doing a 

39
00:02:44.115 --> 00:02:47.781
naive grid search. 
We can just sample random combinations. 

40
00:02:47.781 --> 00:02:53.288
That is for each hyper-parameter, we make 
a list of alternatives and then we pick 

41
00:02:53.288 --> 00:02:58.720
one thing randomly from each list. 
The reason that's better is because some 

42
00:02:58.720 --> 00:03:03.446
of the hyper-parameters won't have much 
effect and others will have a lot of 

43
00:03:03.446 --> 00:03:06.369
effect. 
And what we don't want to do is exactly 

44
00:03:06.369 --> 00:03:10.224
repeat the settings of the 
hyper-parameters that have a lot of 

45
00:03:10.224 --> 00:03:14.797
effect for different settings of 
hyper-parameters that don't have much 

46
00:03:14.797 --> 00:03:17.162
effect. 
We don't learn much that way. 

47
00:03:17.162 --> 00:03:22.350
In a grid search, you'll have several 
points along each axis that are identical 

48
00:03:22.350 --> 00:03:26.882
for all the other parameters. 
And so, if moving along that axis of the 

49
00:03:26.882 --> 00:03:32.202
grid search makes no difference, you've 
replicated the same experiment many times 

50
00:03:32.202 --> 00:03:36.220
and haven't learned anything about the 
other parameters. 

51
00:03:36.220 --> 00:03:40.548
There's something you can do that's much 
better than random combinations, 

52
00:03:40.548 --> 00:03:45.173
and basically it amounts to saying, let's 
use machine learning to simulate the 

53
00:03:45.173 --> 00:03:49.680
graduate student who is trying to decide 
what the hyper-parameter should be. 

54
00:03:51.140 --> 00:03:56.179
So, instead of using random combinations, 
we look at the results we've got so far 

55
00:03:56.179 --> 00:04:00.084
and try and predict what combinations are 
likely to work well. 

56
00:04:00.084 --> 00:04:04.619
That is, we have to predict regions of 
the hyper-parameter space, in which we 

57
00:04:04.619 --> 00:04:09.146
expect to get good results. 
It's not sufficient just to say how well 

58
00:04:09.146 --> 00:04:12.350
we expect to do. 
We also have to have an idea of the 

59
00:04:12.350 --> 00:04:15.246
uncertainty. 
We might, for example, have a region, 

60
00:04:15.246 --> 00:04:19.067
where we expect to do about the same as 
we're currently doing, 

61
00:04:19.067 --> 00:04:23.688
but maybe we would do much better. 
In that case, it would be worth going and 

62
00:04:23.688 --> 00:04:27.509
exploring that region. 
It's even worth exploring regions where 

63
00:04:27.509 --> 00:04:31.500
we expect to do worse, 
but we might just do a lot better. 

64
00:04:31.500 --> 00:04:36.148
Now we're going to assume that the amount 
of computation involved in evaluating one 

65
00:04:36.148 --> 00:04:40.797
setting of the hyper-parameters is huge. 
It involves training a big neura; network 

66
00:04:40.797 --> 00:04:45.040
on a huge data set and it might take 
several days on a big computer. 

67
00:04:45.040 --> 00:04:50.676
Relative to that amount of work, building 
a model to predict how well a setting of 

68
00:04:50.676 --> 00:04:56.037
the hyper-parameters will do, given all 
the settings we've experimented with so 

69
00:04:56.037 --> 00:05:00.024
far is much less work. 
And so it's going to require much less 

70
00:05:00.024 --> 00:05:05.523
computation to fit the predictive model 
to the results of the experiments we've 

71
00:05:05.523 --> 00:05:08.960
seen so far than it is to run a single 
experiment. 

72
00:05:08.960 --> 00:05:13.551
So what kind of model are we going to use 
for predicting the results of future 

73
00:05:13.551 --> 00:05:16.632
experiments? 
It turns out there's a kind of model I 

74
00:05:16.632 --> 00:05:21.440
haven't talked about in the course called 
Gaussian process models. 

75
00:05:21.440 --> 00:05:26.696
Basically, all these models do is assume 
that similar inputs give similar outputs. 

76
00:05:26.696 --> 00:05:30.265
They don't have any more sophisticated 
prior than that, 

77
00:05:30.265 --> 00:05:34.288
but they're very good at using that prior 
in an effective way. 

78
00:05:34.288 --> 00:05:39.285
So if you don't know much about what you 
expect hyper-parameters could do, a weak 

79
00:05:39.285 --> 00:05:42.740
prior like that is probably the best you 
can do. 

80
00:05:42.740 --> 00:05:47.836
Gaussian processes are able to learn for 
each input dimension what the appropriate 

81
00:05:47.836 --> 00:05:52.374
scale is for measuring similarity. 
So for example, if the number of hidden 

82
00:05:52.374 --> 00:05:57.098
units could be 200 or it could 300, 
the question is are those similar number 

83
00:05:57.098 --> 00:06:01.884
or are those very different numbers? 
Should we expect the results we get with 

84
00:06:01.884 --> 00:06:06.981
200 to be very similar to the results we 
get with 300 or should we expect them to 

85
00:06:06.981 --> 00:06:11.514
be very different? 
If we don't know anything about neural 

86
00:06:11.514 --> 00:06:17.214
nets, initially we have no idea, but we 
could look at the results of experiments 

87
00:06:17.214 --> 00:06:20.578
so far. 
And if experiments with 200 and 

88
00:06:20.578 --> 00:06:25.385
experiments with 300 tend to give very 
similar answers when you take into 

89
00:06:25.385 --> 00:06:28.892
account the other differences between the 
experiments, 

90
00:06:28.892 --> 00:06:33.894
then 200 is probably similar to 300. 
And so, we set a scale for that dimension 

91
00:06:33.894 --> 00:06:38.831
such that you need differences of much 
more than that to expect to get very 

92
00:06:38.831 --> 00:06:41.949
different results. 
Now, it's important that Gaussian 

93
00:06:41.949 --> 00:06:46.430
processes models do more than just 
predicting the expected outcome of a 

94
00:06:46.430 --> 00:06:50.959
particular experiment. 
That is how well the neural net that we 

95
00:06:50.959 --> 00:06:56.032
train will do on a validation set. 
In addition to predicting a mean value 

96
00:06:56.032 --> 00:07:00.548
for how well they expect the neural 
network to do, they predict a 

97
00:07:00.548 --> 00:07:03.540
distribution, 
they predict the variance. 

98
00:07:03.540 --> 00:07:09.260
They're called Gaussian processes because 
their predictions are Gaussian. 

99
00:07:09.260 --> 00:07:14.213
When they're making a prediction for new 
settings of hyper-parameters that are 

100
00:07:14.213 --> 00:07:19.354
close to several consistent settings that 
we've already run, so we know the answer. 

101
00:07:19.354 --> 00:07:23.994
The predictions will tend to be fairly 
sharp, that is well have low variance, 

102
00:07:23.994 --> 00:07:28.571
but when they are predictions for 
experiments with hyper-parameters that 

103
00:07:28.571 --> 00:07:33.399
are very different from in setting the 
hyper-parameters we'd experimented it 

104
00:07:33.399 --> 00:07:36.722
with so far, 
the predictions made by Gaussian process 

105
00:07:36.722 --> 00:07:42.815
models will have very high variance. 
So here's quite a good strategy for using 

106
00:07:42.815 --> 00:07:47.140
Gaussian processes to decide what to try 
next. 

107
00:07:47.140 --> 00:07:51.836
So remember, we have one kind of learning 
model, which is a big neural network that 

108
00:07:51.836 --> 00:07:55.559
takes a long time to route, 
and we're trying to figure out a good 

109
00:07:55.559 --> 00:07:58.080
setting of the hyper-parameters to try 
next. 

110
00:07:58.080 --> 00:08:02.434
We have a different kind of machine 
learning algorith, called a Gaussian 

111
00:08:02.434 --> 00:08:07.492
process, that's looking at the results of 
the experiments we've done so far and 

112
00:08:07.492 --> 00:08:12.679
trying to predict for some proposed new 
setting of the hyper-parameters How well 

113
00:08:12.679 --> 00:08:16.969
the neural network would do and also how 
unsure that prediction is? 

114
00:08:16.969 --> 00:08:22.027
So what we're going to do is we're going 
to keep track of the hyper-parameters 

115
00:08:22.027 --> 00:08:26.317
that have worked best so far. 
That is a single setting of all the 

116
00:08:26.317 --> 00:08:31.120
hyper-parameters that gave us the neural 
net with the highest performance so far. 

117
00:08:31.120 --> 00:08:36.722
Now, when we run the next experiment, our 
best setting so far might be replaced by 

118
00:08:36.722 --> 00:08:42.116
the new experiment because it gives 
better performances in neural net or it 

119
00:08:42.116 --> 00:08:46.197
might stay the same. 
So since we're going to substitute the 

120
00:08:46.197 --> 00:08:51.384
results of the new experiment it is 
better than anything we've seen so far, 

121
00:08:51.384 --> 00:08:57.480
our best setting so far can only improve. 
So here's a good strategy for what 

122
00:08:57.480 --> 00:09:01.320
setting of the hyper-parameters to try 
next. 

123
00:09:01.320 --> 00:09:05.164
We pick a setting of the 
hyper-parameters, 

124
00:09:05.164 --> 00:09:11.220
such that the expected improvement in our 
best setting is big. 

125
00:09:11.220 --> 00:09:16.381
We don't worry about the fact that we 
might do an experiment that leads to a 

126
00:09:16.381 --> 00:09:20.202
really bad result, 
because if it gets a really bad result, 

127
00:09:20.202 --> 00:09:24.090
we won't replace our best so far with 
this new experiment. 

128
00:09:24.090 --> 00:09:29.029
Also, when learn something. 
This is a phenomenon that managers of 

129
00:09:29.029 --> 00:09:34.651
hedge funds know about. 
I often tell the client if the fund goes 

130
00:09:34.651 --> 00:09:41.000
up, I'll take 3% of your profits. 
If the fund goes down, you lose. 

131
00:09:41.000 --> 00:09:46.388
Now that's a crazy thing for a client to 
agree to, because that gives the hedge 

132
00:09:46.388 --> 00:09:51.846
fund manager huge incentive for taking 
huge risks, because he has no significant 

133
00:09:51.846 --> 00:09:54.915
answer. 
But, for finding hyper-parameters that 

134
00:09:54.915 --> 00:09:59.895
work well, it's a sensible strategy. 
So, consider these three predictions, A, 

135
00:09:59.895 --> 00:10:02.828
B, and C. 
We're going to suppose that A, B, and C 

136
00:10:02.828 --> 00:10:08.000
are different settings of the 
hyper-parameters that have not been tried 

137
00:10:08.000 --> 00:10:13.524
and those green Gaussians are the 
prediction of our Gaussian process model 

138
00:10:13.524 --> 00:10:16.722
for how well each of those setting would 
do. 

139
00:10:16.722 --> 00:10:22.391
For setting A, the mean is well below our 
current best so far and there's only 

140
00:10:22.391 --> 00:10:28.495
moderate variance. 
For setting B, the mean is closer to our 

141
00:10:28.495 --> 00:10:32.710
best so far, 
but since there isn't much variance, 

142
00:10:32.710 --> 00:10:38.721
there really isn't that much upside. 
For setting C, the mean is actually lower 

143
00:10:38.721 --> 00:10:43.780
than for setting B, but because it's high 
variance there's a big upside. 

144
00:10:43.780 --> 00:10:48.981
We're going to take the area under 
Gaussian C that's above the red line and 

145
00:10:48.981 --> 00:10:54.528
we're going to take the moment of that 
area above the red line and that's the 

146
00:10:54.528 --> 00:10:59.681
thing we're looking for the matching 
margin and you can see that C has a much 

147
00:10:59.681 --> 00:11:04.481
bigger moment than B or A. 
It may only have the same area as B above 

148
00:11:04.481 --> 00:11:07.728
the line, 
but some of that area is much further 

149
00:11:07.728 --> 00:11:11.681
above the line, 
so we might get a very big win if we try 

150
00:11:11.681 --> 00:11:15.210
setting C. 
So that's the one our policy would tell 

151
00:11:15.210 --> 00:11:18.600
us to pick here. 
Here's the worst part, 

152
00:11:18.600 --> 00:11:25.380
B is intermediate and c is the best bet. 
So how well does this work? 

153
00:11:25.380 --> 00:11:30.580
Well, if you got the resources to run a 
lot of experiments, it's much better than 

154
00:11:30.580 --> 00:11:34.610
a person of finding good combinations of 
the hyper-parameters. 

155
00:11:34.610 --> 00:11:39.420
The policy I gave you so far is a 
strictly sequential policy that assumes 

156
00:11:39.420 --> 00:11:42.670
that it can see all of the experiments 
run so far, 

157
00:11:42.670 --> 00:11:47.895
but there's no reason why you shouldn't 
make it a bit more complicated and run a 

158
00:11:47.895 --> 00:11:53.120
whole bunch of experiments in parallel. 
Using a Gaussian process model to predict 

159
00:11:53.120 --> 00:11:57.829
how well a particular setting of the 
hyper-parameters will do is sensible, 

160
00:11:57.829 --> 00:12:00.926
because it's not the kind of task we're 
good at. 

161
00:12:00.926 --> 00:12:05.506
It's not like visional speech, and it's 
not clear that there's a lot of 

162
00:12:05.506 --> 00:12:08.474
complicated structure to be found in the 
data. 

163
00:12:08.474 --> 00:12:13.892
It may be that the only real structure is 
that things are smooth and they have some 

164
00:12:13.892 --> 00:12:17.308
scale. 
Also, a person can't keep in mind the 

165
00:12:17.308 --> 00:12:20.785
results of 50 different experiments, to 
see what they predict. 

166
00:12:20.785 --> 00:12:25.060
If you're doing all this by hand, you 
might just fail to notice that all of 

167
00:12:25.060 --> 00:12:29.791
your good results had very small learning 
rates, and all of your really bad results 

168
00:12:29.791 --> 00:12:33.781
had very big learning rates, 
because you're attending to lots of other 

169
00:12:33.781 --> 00:12:38.669
things that you're varying. 
A Gaussian process model would not miss a 

170
00:12:38.669 --> 00:12:42.358
trend like that. 
One final reason why Gaussian process 

171
00:12:42.358 --> 00:12:47.550
models are a very good way of setting 
hyper-parameters is they're much less 

172
00:12:47.550 --> 00:12:52.339
likely than a person to cheat. 
Typically when we're doing research, we 

173
00:12:52.339 --> 00:12:57.004
want to compare a new method that we 
thought of with some old or standard 

174
00:12:57.004 --> 00:12:59.842
method, 
and there's a very strong tendency to 

175
00:12:59.842 --> 00:13:04.886
work harder to find good hyperparameters 
for our new method than for the stupid 

176
00:13:04.886 --> 00:13:08.365
old method. 
That's why when you compare methods, you 

177
00:13:08.365 --> 00:13:11.882
should really compare the results got by 
different groups, 

178
00:13:11.882 --> 00:13:16.551
where for each method, the results are 
produced by the group that believes in 

179
00:13:16.551 --> 00:13:19.522
that method. 
If we use Gaussian process models to 

180
00:13:19.522 --> 00:13:24.312
search for good sets of hyper-parameters, 
they're going to do just as hard a search 

181
00:13:24.312 --> 00:13:29.285
for the type of model we don't believe in 
as they are for the type of model we do 

182
00:13:29.285 --> 00:13:29.952
believe in. 