In this video, I'm going to talk about 
some exciting recent work, which I think will go a long way towards answering the 
question how do you settle those hyper-parameters in a neural network? 
This recent work uses a different kind of machine learning to help us decide what 
values to use for hyper-parameters. In other words, it's using machine 
learning to replace the graduate student who fiddles around with all these 
different settings of the hyper-parameters to find out what works. 
It relies on a way of modeling smooth functions called Gaussian processes, 
which I had always thought of as inadequate for doing things like speech 
and vision and I still think they are inadequate for that. 
But when you're in a domain where you don't have much prior knowledge and the 
only thing that you can really appeal to is that you expect similar inputs to have 
similar outputs, then Gaussian processes are ideal. 
And that's the domain we're in when we're fiddling around with vectors of 
hyper-parameters hoping to find a vector of hyper-parameters that works well. 
So, for example is the number of hidden units, the number of layers, the weight 
penalty, whether it's used drop out or not. Those 
are all hyper-parameters and different combinations of them work well together. 
So this is a very hard space to explore by hand. 
It's very easy when we're exploring by hand to fail to notice things. 
Gaussian processes are very good at noticing trends in the data and they 
provide a very good way of finding good sets of hyper-parameters if you have 
enough computers. One of the commonest reasons that people 
give for not using neural networks is that it requires a lot of skill to set 
the hyper-parameters. This is actually a pretty good reason. 
If you don't have much experience, it's easy to get stuck using a completely 
wrong value for one of the hyper-parameters, and then nothing works. 
You have to set things like the number of layers, 
the number of units per layer, what types of units to use, 
the weight penalty, the learning rate, the momentum, and so on and so on. 
If you use a learning rate that's 100 times too big or 100 times too small, 
your network simply won't work. One way to approach this is to do a naive 
grid search. That is, for each of these 
hyper-parameters, you make a list of alternative values and then you try all 
possible combinations of values. You can see that this is going to blow 
up. If you have more than a few 
hyper-parameters, you're going to end up with many more 
combinations than you can possibly try. It turns out that there's something 
that's considerably better than doing a naive grid search. 
We can just sample random combinations. That is for each hyper-parameter, we make 
a list of alternatives and then we pick one thing randomly from each list. 
The reason that's better is because some of the hyper-parameters won't have much 
effect and others will have a lot of effect. 
And what we don't want to do is exactly repeat the settings of the 
hyper-parameters that have a lot of effect for different settings of 
hyper-parameters that don't have much effect. 
We don't learn much that way. In a grid search, you'll have several 
points along each axis that are identical for all the other parameters. 
And so, if moving along that axis of the grid search makes no difference, you've 
replicated the same experiment many times and haven't learned anything about the 
other parameters. There's something you can do that's much 
better than random combinations, and basically it amounts to saying, let's 
use machine learning to simulate the graduate student who is trying to decide 
what the hyper-parameter should be. So, instead of using random combinations, 
we look at the results we've got so far and try and predict what combinations are 
likely to work well. That is, we have to predict regions of 
the hyper-parameter space, in which we expect to get good results. 
It's not sufficient just to say how well we expect to do. 
We also have to have an idea of the uncertainty. 
We might, for example, have a region, where we expect to do about the same as 
we're currently doing, but maybe we would do much better. 
In that case, it would be worth going and exploring that region. 
It's even worth exploring regions where we expect to do worse, 
but we might just do a lot better. Now we're going to assume that the amount 
of computation involved in evaluating one setting of the hyper-parameters is huge. 
It involves training a big neura; network on a huge data set and it might take 
several days on a big computer. Relative to that amount of work, building 
a model to predict how well a setting of the hyper-parameters will do, given all 
the settings we've experimented with so far is much less work. 
And so it's going to require much less computation to fit the predictive model 
to the results of the experiments we've seen so far than it is to run a single 
experiment. So what kind of model are we going to use 
for predicting the results of future experiments? 
It turns out there's a kind of model I haven't talked about in the course called 
Gaussian process models. Basically, all these models do is assume 
that similar inputs give similar outputs. They don't have any more sophisticated 
prior than that, but they're very good at using that prior 
in an effective way. So if you don't know much about what you 
expect hyper-parameters could do, a weak prior like that is probably the best you 
can do. Gaussian processes are able to learn for 
each input dimension what the appropriate scale is for measuring similarity. 
So for example, if the number of hidden units could be 200 or it could 300, 
the question is are those similar number or are those very different numbers? 
Should we expect the results we get with 200 to be very similar to the results we 
get with 300 or should we expect them to be very different? 
If we don't know anything about neural nets, initially we have no idea, but we 
could look at the results of experiments so far. 
And if experiments with 200 and experiments with 300 tend to give very 
similar answers when you take into account the other differences between the 
experiments, then 200 is probably similar to 300. 
And so, we set a scale for that dimension such that you need differences of much 
more than that to expect to get very different results. 
Now, it's important that Gaussian processes models do more than just 
predicting the expected outcome of a particular experiment. 
That is how well the neural net that we train will do on a validation set. 
In addition to predicting a mean value for how well they expect the neural 
network to do, they predict a distribution, 
they predict the variance. They're called Gaussian processes because 
their predictions are Gaussian. When they're making a prediction for new 
settings of hyper-parameters that are close to several consistent settings that 
we've already run, so we know the answer. The predictions will tend to be fairly 
sharp, that is well have low variance, but when they are predictions for 
experiments with hyper-parameters that are very different from in setting the 
hyper-parameters we'd experimented it with so far, 
the predictions made by Gaussian process models will have very high variance. 
So here's quite a good strategy for using Gaussian processes to decide what to try 
next. So remember, we have one kind of learning 
model, which is a big neural network that takes a long time to route, 
and we're trying to figure out a good setting of the hyper-parameters to try 
next. We have a different kind of machine 
learning algorith, called a Gaussian process, that's looking at the results of 
the experiments we've done so far and trying to predict for some proposed new 
setting of the hyper-parameters How well the neural network would do and also how 
unsure that prediction is? So what we're going to do is we're going 
to keep track of the hyper-parameters that have worked best so far. 
That is a single setting of all the hyper-parameters that gave us the neural 
net with the highest performance so far. Now, when we run the next experiment, our 
best setting so far might be replaced by the new experiment because it gives 
better performances in neural net or it might stay the same. 
So since we're going to substitute the results of the new experiment it is 
better than anything we've seen so far, our best setting so far can only improve. 
So here's a good strategy for what setting of the hyper-parameters to try 
next. We pick a setting of the 
hyper-parameters, such that the expected improvement in our 
best setting is big. We don't worry about the fact that we 
might do an experiment that leads to a really bad result, 
because if it gets a really bad result, we won't replace our best so far with 
this new experiment. Also, when learn something. 
This is a phenomenon that managers of hedge funds know about. 
I often tell the client if the fund goes up, I'll take 3% of your profits. 
If the fund goes down, you lose. Now that's a crazy thing for a client to 
agree to, because that gives the hedge fund manager huge incentive for taking 
huge risks, because he has no significant answer. 
But, for finding hyper-parameters that work well, it's a sensible strategy. 
So, consider these three predictions, A, B, and C. 
We're going to suppose that A, B, and C are different settings of the 
hyper-parameters that have not been tried and those green Gaussians are the 
prediction of our Gaussian process model for how well each of those setting would 
do. For setting A, the mean is well below our 
current best so far and there's only moderate variance. 
For setting B, the mean is closer to our best so far, 
but since there isn't much variance, there really isn't that much upside. 
For setting C, the mean is actually lower than for setting B, but because it's high 
variance there's a big upside. We're going to take the area under 
Gaussian C that's above the red line and we're going to take the moment of that 
area above the red line and that's the thing we're looking for the matching 
margin and you can see that C has a much bigger moment than B or A. 
It may only have the same area as B above the line, 
but some of that area is much further above the line, 
so we might get a very big win if we try setting C. 
So that's the one our policy would tell us to pick here. 
Here's the worst part, B is intermediate and c is the best bet. 
So how well does this work? Well, if you got the resources to run a 
lot of experiments, it's much better than a person of finding good combinations of 
the hyper-parameters. The policy I gave you so far is a 
strictly sequential policy that assumes that it can see all of the experiments 
run so far, but there's no reason why you shouldn't 
make it a bit more complicated and run a whole bunch of experiments in parallel. 
Using a Gaussian process model to predict how well a particular setting of the 
hyper-parameters will do is sensible, because it's not the kind of task we're 
good at. It's not like visional speech, and it's 
not clear that there's a lot of complicated structure to be found in the 
data. It may be that the only real structure is 
that things are smooth and they have some scale. 
Also, a person can't keep in mind the results of 50 different experiments, to 
see what they predict. If you're doing all this by hand, you 
might just fail to notice that all of your good results had very small learning 
rates, and all of your really bad results had very big learning rates, 
because you're attending to lots of other things that you're varying. 
A Gaussian process model would not miss a trend like that. 
One final reason why Gaussian process models are a very good way of setting 
hyper-parameters is they're much less likely than a person to cheat. 
Typically when we're doing research, we want to compare a new method that we 
thought of with some old or standard method, 
and there's a very strong tendency to work harder to find good hyperparameters 
for our new method than for the stupid old method. 
That's why when you compare methods, you should really compare the results got by 
different groups, where for each method, the results are 
produced by the group that believes in that method. 
If we use Gaussian process models to search for good sets of hyper-parameters, 
they're going to do just as hard a search for the type of model we don't believe in 
as they are for the type of model we do believe in.