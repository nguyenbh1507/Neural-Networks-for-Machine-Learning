. 
In this video I'm going to show how we can first learn a deep belief net by 
stacking up restricted Boltzmann machines. 
And then we can treat that as a deep neural net that we fine tune 
discriminatory. So instead of fine tuning it to be better 
at generation, as we did in the previous video, we're going to fine tune it to be 
better at discriminating between classes. This works very well and led to a big 
renewal of interest in neural networks. In speech recognition, it's had a major 
influence and many leading groups are now switching to using deep neural nets in 
order to reduce the error rate, in speech recognition. 
I now want to talk about fine tuning these deep networks to be better at 
discrimination. So we first learn one layer of features 
at a time, by stacking up restricted Boltzmann machines. 
Then we treat this as pre-training that finds a good initial set of weights in 
the DPO network and we fine tune those weights using some local search 
procedure. In the previous video I showed you how to 
use contrustive weight sleep to fine tune a deep network so that it was better 
generating its inputs. In this video we're going to use back 
propagation to fine tune a model to be better at discrimination. 
If we do this it overcomes many of the standard limitations of back propagation. 
It makes it much easier to learn deep nets. 
And it makes those nets generalise better. 
We need to understand why back propagation when we pre-train the 
weights. And there's really two effects. 
There's an effect on optimization and there's an effect on generalization. 
So the pre-training scales really well if we have big networks, especially if each 
layer has locality. So if we're doing vision, for example, 
and we had local receptor fields in each layer, then there's not much interaction 
between widely separate locations. And so it's very easy to learn a big 
layer more or less in parallel. When we do pre-training. 
We don't start back propagation until we've already learned sensible feature 
detectors. And these feature detectors should be 
very helpful for discrimination. So the initial gradients are much more 
sensible than if we use random ones. And back propagation doesn't need to do a 
global search. It just needs to do a local search from a 
sensible starting point. In addition to being easier to optimize, 
pre-trained nets exhibit much less overfitting. 
That's because most of the information in the final weights comes from modeling the 
distribution of input vectors. And these input vectors, if you're 
dealing with something like images, generally contain a lot more information 
than labels. A label typically only contains a few 
bits of information to constrain the mapping from input to output. 
Whereas an image contains a lot of information which will constrain any 
generative model of a set of images. The information in the labels is only 
used for the final fine tuning And because by that stage we've already 
decided on the feature detectors, we're not squandering that precious information 
designing feature detectors from scratch. The fine tuning only makes slight changes 
to the feature detectors we learned in the generative pre-training phase. 
And those are the changes required to get the category boundaries in the right 
place. The important thing is the back 
propagation is not being required to discover new features. 
And so it doesn't need nearly as much label data. 
In fact, this type of learning works well when most of the data is unlabeled, 
because the generative pre-training can make use of the light data. 
The unlabeled data is still very useful for discovering good features. 
There is an obvious objection to this type of learning, which is that when we 
do generative pre-training. We'll be learning lots of features that 
are useless for the particular discriminative task we want the net to 
do. Consider, for example, that you might 
want the net to discriminate between shapes or you might want the net to 
discriminate between different poses of one shape. 
They need very different features, and if you don't know the task in advance. 
You'll inevitably learn features that are never used. 
When computers were much smaller, that was the serious objection. 
But now that computers are large enough, we can afford to learn features that are 
never used. And, we can afford it because among all 
the features we learn, there will be some that are much more useful than their raw 
inputs. And that more than makes up for the fact 
that we have learned some features that aren't helpful for the particular task 
we're interested in. So let's apply this to modeling the 
m-list digits. We'll now learn three hidden layers of 
features entirely unsupervised. Once we've done that learning, when we 
generate from the model, it will generate things that look like 
real digits. And it'll generate them from all the 
different classes. And it'll typically take a while before 
it switches from one class to another. And it will typically take a while before 
it switches from one class to another because it'll tend to stay in the same 
ravine for a while before it jumps to another ravine. 
But the question is, are the features that we've learned that way useful for 
doing discrimination? So all we need to do is add a final 
10-way soft max at the top. And fine tune it with back propagation. 
And see if we do better than purely discriminative training. 
So here's the results on the permutation invariant M-ness task. 
And what I mean is permutation invariant is, if we were to apply a fixed random 
permutation to all the pixels, the same permutation to every test and training 
case, the results of our algorithm wouldn't change. 
That's clearly not true for something like a convolutional net. 
A convolutional net's been told something about the task. 
By applying this fixed permutation, we destroy all simple ways of telling the 
net something about the spatial nature of the task. 
So if you apply standard back propagation. 
It's hard to do better than 1.6% errors. John Platt and myself have both tried 
quite hard applying standard back propagation with various different 
architectures. And we're both quite good at doing it. 
You can actually beat 1.6%. By using constraints on the incoming 
weight vectors of the hidden units. If you use an appropriate restriction on 
the length of an incoming weight vector, you can do a bit better than 1.6%. 
Support vector machines can get 1.4 percent. 
And this was one of the pieces of evidence that led to support vector 
machines, supplanting back propagation. If you pretrain a network using a stack 
of Boltzmann Machines. And then you fine tune it to be better at 
generating the joint density of digits and image labels. 
Then you can get down to 1.25%. If you train a stack of Boltzmann 
machines, and simply put a 10-way [INAUDIBLE] on top, and fine tune it. 
You can get to 1.15%. And with more fiddling around, you can 
get that down to about one%. So you can do a lot better than standard 
back propagation. And also better than support vector 
machines by using generative pre-training followed by discriminative fine tuning. 
Mackerie Yerenzato working in Yan LeCanne's group also showed, using a 
slightly different pre-training method, that pre-training helps for models that 
have more data and better prions. So they used an additional 60,000 
distorted digital images. So they had a lot more training data. 
They also used convolution multilinear network. 
And Yan's group is the best group, at tuning those. 
With back propagation, they managed to get down to.49%. 
When they did the unsupervised layer by layer pre-training, followed by back 
propagation they got down to.39%. Which at the time was a record. 
So you may remember this picture from the first lecture. 
This was one of the examples I gave of the successive neural nets. 
It's the same picture. Back then, I said we could get down to 
20.7% by pre-training and then fine tuning with back propagation, and that 
the previous, and that the previous speak independent record on Timint was 24.4%. 
Which actually required averaging several models. 
Lee Ding at Microsoft Research picked up at this result immediately and 
collaborated on improving it. And this has led to a big change in 
speech recognition. If you look at this news story, it will 
refer you to a blog where the chief research officer for Microsoft is talking 
about the big improvements in speech recognition caused by using deep neural 
nets.