In this video I'm going to describe how 
to use an RBM to model real value data. The idea is that we make the visible 
units. Instead of being binary stochastic units, 
the linear units with Gaussian noise. When we do this, we get problems with 
learning. And it turns out a good solution to those 
problems is to then make the hidden units be rectified linear units. 
With linear Gaussian units for the visible, and rectified linear units for 
the hiddens, it's quite easy to learn a restricted Boltzmann machine that makes a 
good model of real value data. We first used restricted Boltzmann 
machines with the images of handwritten digits. 
For those images. Intermediate intensities caused by a 
pixel being only partially inked can be modelled quite well by probabilities, 
that is numbers between one and zero that are actually the probability of a 
logistical unit being on. So we treat partially inked pixels. 
As having a probability of being inked. This is incorrect but it works quite 
well. However it won't work for real images. 
In a real image the intensity of a pixel is almost always, almost exactly the 
average of its neighbors. So its got a very high probability of 
being very close to that average and a very small probability of being a little 
further away. And you can't achieve that with a 
logistic unit. Mean field logistic units are unable to 
represent things like the intensity is 69. but very unlikely to be 71. or 67. So 
we need some other kind of unit. The obvious thing to use is a linear unit 
with Gaussian norms. So we model pixels as Gaussian variables. 
We can still use alternating, get sampling, to run the Markoff chain 
required for the cross-divergence learning. 
But we need to use a much smaller learning range, otherwise it will tend to 
blow up. The equation looks like this. 
The first term on the right hand side, is a kind of parabolic containing function. 
It stops things blowing out. So determining that sum contributed by 
the Ith visible unit is parabolic in shape. 
It looks like this. It's parabola with its minimum at the 
bias of the Ith unit. And as the Ith unit departs from that 
value, we add energy quadratically. So that tries to keep the Ith visible 
unit close to VI. The interactive term between the visible 
and the hidden units looks like this. And if you differentiate that with 
respect to the I, you can see that you get a constant. 
It's the sum over all J, of H J W I J divided by sigma I. 
So that term with its constant gradient looks like this. 
And when you add together, that top down contribution to the energy is linear, and 
the parabolic containment function. You'll get a parabolic function, but with 
the mean shifted away from BI. And how much it shifted depends on the 
slope of that blue line. So the effect of the hidden units is just 
to push the mean to one side. It's easy to write down an energy 
function like this. And it's easy to take derivatives off it. 
But when we try learning with it, we often get problems. 
There were a lot of reports in the literature that people could not get 
these Gaussian binary RBM's to work. And it is indeed extremely hard to learn 
tight variances for the visible units. It took us a long time to figure out why 
it's so hard to learn those visible variances. 
This picture helps. If you consider the effect that visible 
unit I has on hidden unit J. When visible unit I has a strong standard 
deviation sigma I, that has the effect of exaggerating the bottom up weights. 
That's because we need to measure the activity of I in units of its standard 
deviation. So when the standard deviation is small, 
we need to multiply the weight by a lot. If you look at the top down effect of J 
on I, that's multiplied by sigma I. So when the standard deviation of a 
visible unit I is very small, the bottom up effects get exaggerated, on the top 
down effects get attenuated. The result is that we have a conflict 
where either we have bottom up effects that are much too big or top down effects 
that are much too small. And the result is that the hidden units 
tend to saturate and be firmly on or off all the time, and this will mess up 
learning. So the solution is to have many more 
hidden units than visible units. That allows small weights between the 
visible and hidden units to have big top down effects, because of so many hidden 
units. But of course, we really need the number 
of hidden units to change as that standard deviation sigma I gets smaller. 
And on the next slide, we'll see how we can achieve that. 
I'm going to introduce stepped sigmoid units. 
The idea is we make many copies of each stacastic binary hidden unit. 
All the copies have the same weights, and the same bias that's learned B But in 
addition to that adapted bias B they have a fixed offset to the bias. 
The first unit has an offset of -1.5. The second unit has an offset of -1.5. The 
third one has an offset of minus -2.5, and so on. 
If you have a whole family of sigmoid units like that, with the bias changed by 
one between neighbouring members of the family, the response code looks like 
this. If the total in product is very low, none 
of them are turned on. As it increases, the number that get 
turned on increases linearly. This means that as the standard deviation 
on the previous slide gets smaller, the number of copies of each hidden unit that 
get turned on gets bigger and we achieved just the effect we wanted, which we get 
more top-down effect to drive these visible units that have small standard 
deviations. Now it's quite expensive to use a big 
population of binary stochastic units with offset biases, because for each one 
of them, we need to put the total input through the logistic function, but we can 
make some fast approximations which work just as well. 
So the sum of the activities of a whole bunch of sigmoid units with offset 
ballasts, which is shown in that summation. 
Is approximately equal to log of one plus E to the X and that in turn is 
approximately equal to the maximum of nought and X. 
And we can add some noise to the X if we want. 
So the first term in the equation looks like this. 
The second term looks like that. And you can see that the sum of all those 
sigmoids in the first term will be a curve like that. 
And we can approximate that by a linear threshold unit that has a value of zero 
unless it's above threshold. In which case its value increases 
linearly with its input. Contrastive Divergence Learning works 
well for the sum of a bunch of stochastic logistic units with offset biases. 
And in that case. You get a noise variance that's equal to 
the logistic function. But the output of that sum. 
Alternatively we can use that green curve and use rectified linear units. 
They're much faster to compute because you don't need to go through the logistic 
many times. And can trust divergence works just fine 
with those. One nice property of rectified linear 
units is that if they have a bias of zero, they exhibit scale equivariance. 
This is a very nice property to have for images. 
What scale equivariance means is that if you take an image x and you multiply all 
the pixel intensities by a scalar a., then the representation of ax in the 
rectified linear units would be just a times the representation of x. 
In other words, when we scale up all the intensities in the image, we scale up the 
activities of all the hidden units but all the ratios stay the same. 
Rectified linear units aren't fully linear because if you add together two 
images, the representation you get is not the sum of the representations of each 
unit separately. This property of scale equivariance is 
quite similar to the property of translational equivariance, convolutional 
nets off. So if we ignore the pooling for now, in a 
convolution on that, if we shift an image and look at the representation, 
the representation of a shifted image is just a shifted version of the 
representation of the unshifted image. So in a convolutional net without 
pooling, translations of the input just flow through the layers of the net 
without really affecting anything. The representation of every layer is just 
translated.