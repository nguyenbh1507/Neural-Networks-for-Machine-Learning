In this video I'm going to talk about 
some advanced material. It's not really appropriate for a first 
course on nerual networks but I know that some of you are particularly interested 
in the urgent of deep learning. And the content of this video is 
mathematically very pretty So I couldn't resist putting it in. 
[INAUDIBLE] insight that stacking up restrictive Boltzmann machines gives you 
something like a sigmoid belief net can actually be seen without doing any math. 
Just by noticing, that a restrictive Boltzmann machine is actually the same 
thing as an infinitely deep sigmoid belief net with shared weights. 
Once again, wave sharing leads to something very interesting. 
I'm now going to describe, a very interesting explanation of why layer by 
layer learning works. It depends on the fact that there is an 
equivalence between restricted bowlser machines, which are undirected networks 
with symmetric connections, and infinitely deep directed networks. 
In which every layer uses the same weight matrix. 
This equivalence also gives insight into why contrasted divergence learning works. 
So an RBM is really just an infinitely deep sigmoid belief net with a lot of 
shared weights. The Markoff chain that we run when we 
want to sample from an RBM can be viewed as exactly the same thing as a sigmoid 
belief net. So here's the picture. 
We have a very deep sigmoid belief net. In fact, infinitely deep. 
We use the same weights at every layer. We have to have all the V layers being 
the same size as each other, and all the H layers being the same size as each 
other. But V and H can be different sizes. 
The distribution generated by this very deep network with replicated weights is 
exactly the equilibrium distribution that you get by alternating between doing P of 
V given H, and P of H given V, where both P of V given H and P of H given V are 
defined by the same weight matrix W. And that's exactly what you do when you 
take a restricted Boltzmann machine, and run a Markhov chain to get a sample from 
the equilibrium distribution. So a top-down pass starting from 
infinitely higher up. In this directed note, is exactly 
equivalent to letting a restricted Boltzmann machine settle to equilibrium. 
But that would define the same distribution. 
The sample you get at v0 if you run this infinite directed note, would be an 
equilibrium sample of the equivalent RBM. Now let's look at inference in an 
infinitely deep sigmoid belief net. So in inference we start at v zero and 
then we have to infer the state of h zero. 
Normally this would be a difficult thing to do because of explaining away. 
If for example hidden units K and J both had big positive weights to visible unit 
I, then we would expect that when we observe that I is on, K and J become 
anti-correlated in the posterior distribution. 
That's explaining a way. However in this net, K and J are 
completely independent of one another when we do inference given V0. 
So the inference is trivial, we just multiply V0 by the transpose of W. 
And put whatever we get through the logistic sigmoid and then sample. 
And that gives us binary states of the units in H0. 
But the question is how could they possible be independent given explaining 
away. The answer to that question is that the 
model above H0 implements what I call a complementary prior. 
It implements a prior distribution over H0 that exactly cancels out the 
correlations in explaining away. So for the example shown, the prior will 
implement positive correlation stream k and j. 
Explain your way will cause negative correlations and those will exactly 
cancel. So what's really going on is that when we 
multiply v0 by the transpose of the weights, we're not just computing the 
light unit term. We're computing the product of a light 
unit term and a prior term. And that's what you need to do to get the 
posterior. It normally comes as a big surprise to 
people. That when you multiply by w transpose, 
it's the product of the prior in the posterior of your computer. 
So what's happening in this net is that the complementary prior implemented by 
all the stuff above H0, exactly counts a lot explaining why it makes inference 
very simple. And that's true at every layer of this 
net so we can do inference for every layer and get an unbiased sample with 
each layer simply by multiplying V0 by W transpose. 
Then once we computed the binary state of H0, we multiple that by W. 
Put that through the logistic sigmoid and sample and that will give use a binary 
state for V1 and so on for all the way up. 
Suggestive generating from this model is equivalent to running the alternating 
mark off chain on a restricted Boltzmann machine to equilibrium. 
Performing inference in this model is exactly the same process in the opposite 
direction. This is a very special kind of sigmoid 
belief net in which inference is as easy as generation. 
So here I've shown the generative weights that define the model, and also their 
transposes, that are the way we do inference. 
And now I what want to show is how we get the Bolton Machine Learning Algorithm out 
of the learning algorithm for directed Sigmoid belief nets. 
So the learning rule for Sigmoid belief net says that we should first get a 
sample from the posterior, that what the Sj and Si are, samples from the posterior 
distribution. And then we should change a weight, the 
generative weight in proportion to the product of the pre activity as J and the 
difference between the [INAUDIBLE] activity as i and the probability of 
turning on i given all the binary states of the ladder Sj is in. 
Now if we ask how do we compute Pi, something very interesting happens. 
If you look at inference in this network on the right, we first infer a binary 
state for H0. Once we've chosen that binary state, we 
then infer a binary state for V1 by multiplying H0 by W, putting the result 
through the logistic, and then sampling. So if you think about how Si1 was 
generated? It was a sample from what we get if we 
put H0 through the weight matrix W and then through the logistic. 
And that's exactly what we'd have to, to in order to compute PiO. 
We'd have to take the binary activities in H0 and going downwards now through the 
green weights, W, we will compute the probability of turning on unit I given 
the binary states of its parents. So the point is, the process that goes 
from H0 to V1 is identical to the process that goes from H0 to V0. 
And so SI1 is an unbiased sample of PI0. That means we can replace it in the 
learning rule. So we end up with a learning rule that 
looks like this, because since we have replicated weights, each of those lines 
is the term in the learning rule that comes from one of those green weight 
matrices. For the first green weight matrix here. 
The learning rule is the presynaptic state Sj0 times the difference between 
the post synaptic state Si0 and the probability that the binary states in H0 
would turn on Si. Which we could call PI0 but a sample with 
that probability is Si1. And so an unbiased estimate of the 
relative, can be got by plugging in Si1 on that first line of the learning rule. 
Similarly for the second weight matrix, the learning rule is SI1 into SJ0 minus 
PJ0 and an unbiased estimate of PJ0 is SJ1. 
And so that's an unbiased testament of the learning rule, for this second weight 
matrix. And if you just keep going for all 
wave-matrices you get this infinite series. 
And all the terms except the very first term and the very last term cancel out. 
And so you end up with the Boltzmann machine learning rule. 
Which is just SJ-zero into Si-zero, minus SI-infinity into SI-infinity. 
So let's go back and look at how we would learn an infinitely deep sigmoid belief 
net. We would start by making all the weight 
matrices the same. So we tie all the weight matrices 
together. And we learn using those tied weights. 
Now that's exactly equivalent to learning a restricted Boltzmann machine. 
The diagram on the right and the diagram on the left are identical. 
We can think of the symmetric arrow in the diagram on the left, as just a 
convenient shorthand for an infinite directed net with tied weights. 
So we first learn that restricted Boltzmann machine. 
Now we ought to learn it using maximum likelihood learning, but actually we're 
just going to use contrasted divergence learning. 
We're going to take a shortcut. Once we've learned the first restricted 
Boltzmann machine, what we could do is we could freeze the bottom level weights. 
We'll freeze the generative weights that define the model. 
We'll also freeze the weights we're going to use for inference to be the transpose 
of those generative weights. So we freeze those weights. 
We keep all the other weights tied together. 
But now we're going to allow them to be different from the weights in the bottom 
layer but they're still all tied together. 
So learning the remaining weights tied together is exactly equivalent to 
learning another restrictive Boltzmann machine. 
Namely a restricted Boltzmann machine with H0 as its visible units, V1 as its 
hidden units. And where the data is the aggregated 
posterior across H0. That is, if we want to sample a data 
vector to train this network, what we do is we put in a real data vector, V 
nought, we do inference through those frozen waits, and we get a binary vector 
at H nought, and we treat that as data for training the next restricted 
Boltzmann machine. And we can go up for as many layers as we 
like. And when we get fed up, we just end up 
with the restrictive Boltzmann machine at the top which is equivalent to saying, 
all the weights in the infinite directed net above there are still tied together, 
but the weights below have now all become different. 
Now an explanation of why the inference procedure was correct, involved the idea 
of a complementary prior created by the weights in the layers above but of 
course, when we change the weights in the layers above, but leave the bottom layer 
of weights fixed, the prior created by those changed weights is no longer 
exactly complementary. So now our inference procedure, using the 
frozen weights in the bottom layer, is no longer exactly correct. 
The good news is, it's nearly always very close to correct and with the incorrect 
inference procedure, we still get a variational bound on the low probability 
of the data. The higher layers have changed because 
they've learned a prior for the bottom hidden layer that's closer to the 
aggregated posterior distribution. And that makes the model better. 
So changing the hidden weights makes the inference that we're doing at the bottom 
hidden layer incorrect, but gives us a better model. 
And if you look at those two effects, we prove that the improvement that you 
get in the variation bound from having a better model is always greater than the 
loss that you get from the inference being slightly incorrect. 
So in this variation bound you win when you learn the lights in hire less, 
assuming that you do it with correct maximizer [INAUDIBLE]. 
So now let's go back to what's happening in contrasted divergence learning. 
We have the infinite net on the right and we have a restricted Boltzmann machine on 
the left. And they're equivalent. 
If we were to do maximum likelihood learning for the restricted Boltzmann 
machine, it would be maximum likelihood learning for the infinite sigmoid belief 
net. But what we're going to do is we're going 
to cut things off. We're going to ignore the small 
derivitives for the weights you get in the higher layers of the infinite sigmoid 
belief net. So, we cut it off were that dotted red 
line is. And now if we look at the derivatives, 
the derivatives we're going to get look like this. 
They've got two terms. The first term comes from that bottom 
layer of nets. We've seen that before, the router for 
the bottom layer of weights is just that first line here. 
The second line comes from the next layer of lights. 
That's this line here. We need to compute the activities in H1, 
in order to compute the Sj1 in that second line but we're not actually 
computing derivatives for the third layer of weights. 
And when we take those first two terms, and we combine them. 
We get exactly the learning rule for one step contrasted divergence. 
So what's going on in contrasted divergence, is we're combining weight 
derivatives for the lower layers, and ignoring the weight derivatives in higher 
layers. The question is, why can we get away with 
ignoring those higher derivatives? When the weights are small, the Markov 
chain mixes very fast. If the weights are zero, it mixes in one 
step. And if the Markoff chain mixes fast, the 
higher layers will be close to the equilibrium distribution, i.e. 
They will have forgotten what the input was at the bottom layer. 
And now we have a nice property. If the higher layers are sampled from the 
equilibrium distribution, we know that the derivatives of the log probability, 
the data with respect to the weights, must average out to zero. 
And that's because the current weights in the model are a perfect model of the 
equilibrium distribution. The equilibrium distribution is generated 
using those weights. And if you want to generate samples from 
the equilibrium distribution, those are the best possible weights you could have. 
So we know the root is there is zero. As the weights get larger, we might have 
to run more iterations of Contrastive Divergence. 
Which corresponds to taking into account more layers of that infinite sigmoid 
belief net. That will allow Contrasive Divergence to 
continue to be a good approximation to maximum likelihood and so if we're trying 
to learn a density model, that makes a lot of sense. 
As the weights grow, you run CD for more and more steps. 
If there's a statistician around, you give him a guarantee, then in the 
infinite limit, you'll run CD for infinite many steps. 
And then you have an asymptotic convergence result, which is the thing 
that keeps statisticians happy. Of course it's completely irrelevant 
because you'll never reach a point like that. 
There is however an interesting point here. 
If our purpose in using CD is to build a stack of restricted Boltzmann machines, 
that learn multiple layers of features, it turns out that we don't need a good 
approximation to maximum likelihood. For learning multiple layers of features, 
CD1 is just fine. In fact it's probably better than doing 
maximum l likelihood.